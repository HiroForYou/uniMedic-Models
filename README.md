# uniMedic-Models

Los modelos de uniMedic son los m√≥dulos de PyTorch üî• que se utilizan actualmente en uniMedic-Core, entrenados en conjuntos de datos disponibles p√∫blicamente. Cada modelo tiene una breve explicaci√≥n a continuaci√≥n y un ejemplo de c√≥mo hacer inferencias sobre el modelo. 

### Visual Question Answering ‚ùì

Visual Question Answering es una tarea desafiante para el aprendizaje autom√°tico moderno. Requiere un sistema de inteligencia artificial que pueda comprender tanto el texto como el idioma, de modo que pueda responder preguntas basadas en texto dado el contexto visual (una imagen, una tomograf√≠a computarizada, una resonancia magn√©tica, etc.).

Los investigadores est√°n desarrollando continuamente muchos m√©todos sofisticados para VQA, como [Oscar](https://www.microsoft.com/en-us/research/publication/oscar-object-semantics-aligned-pre-training-for-vision-language-tasks/) √≥ [VL-BERT](https://www.microsoft.com/en-us/research/publication/vl-bert-pre-training-of-generic-visual-linguistic-representations/).

Los enfoques basados en redes neuronales normalmente requieren grandes cantidades de datos. Debido al hecho de que los datos m√©dicos son escasos, desarrollar modelos neuronales para la medicina es mucho m√°s dif√≠cil: los modelos deben ser precisos utilizando peque√±as cantidades de datos.

Nuestro motor VQA se basa en MedVQA, un modelo de vanguardia entrenado en im√°genes y preguntas m√©dicas, utilizando Meta-Learning y un Autoencoder convolucional para la extracci√≥n de representaciones, como se presenta [aqu√≠](https://github.com/aioz-ai/MICCAI19-MedVQA/tree/c076f2cc174def26fa597fce4235b93f56658cc8).

A continuaci√≥n se muestra la arquitectura del modelo VQA:

<p align="center">
  <img align="center" src="https://github.com/aioz-ai/MICCAI19-MedVQA/raw/c076f2cc174def26fa597fce4235b93f56658cc8/misc/Medical_VQA_Framework.png" alt="Arquitectura MedVQA" width="60%">
</p>

### Segmentaci√≥n Cerebral M√©dica üß†

La segmentaci√≥n m√©dica es la tarea de resaltar una regi√≥n o un conjunto de regiones con una propiedad espec√≠fica. Si bien esta tarea se resuelve principalmente en la configuraci√≥n de prop√≥sito general, en la escena m√©dica esta tarea es bastante dif√≠cil debido a la dificultad del problema, los humanos tienen una mayor tasa de error al resaltar anomal√≠as en el cerebro y la falta de datos.

Nuestro modelo usa una arquitectura [UNet](https://arxiv.org/pdf/1505.04597.pdf), una red residual basada en downsampling y upsampling  que tiene un buen desempe√±o en la localizaci√≥n de diferentes caracter√≠sticas, como se presenta en PyTorch [hub](https://pytorch.org/hub/mateuszbuda_brain-segmentation-pytorch_unet/), gracias al trabajo de Mateusz Buda.

La imagen de entrada es una imagen de resonancia magn√©tica cerebral de 3 canales, la salida es un mapa de probabilidad de un canal de regiones anormales, que luego se transforma en una m√°scara de segmentaci√≥n binaria.

A continuaci√≥n se muestra la arquitectura y una peque√±a demostraci√≥n de UNET en una tarea de segmentaci√≥n cerebral:

<p align="center">
  <img align="center" src="misc/unetArq.png" alt="Arquitectura Unet" width="60%">
</p>

<p align="center">
  <img align="center" src="misc/unetDemo.gif" alt="Unet en acci√≥n" width="60%">
</p>


### Etiquetado M√©dico üìù

El etiquetado m√©dico es la tarea de elegir qu√© tipo de imagen el usuario est√° introduciendo en la aplicaci√≥n, las posibles etiquetas son cerebro, pecho, mama, ojos, coraz√≥n, codo, antebrazo, mano, h√∫mero, hombro, mu√±eca. Actualmente, nuestro modelo VQA tiene soporte solo para cerebro y pecho (tor√°x), pero estamos trabajando para agregar soporte a m√∫ltiples etiquetas.

Nuestro modelo utiliza una arquitectura Densenet121 disponible en m√≥dulo [torchvision](https://pytorch.org/docs/stable/torchvision/models.html), la arquitectura ha demostrado ser mejor en im√°genes m√©dicas por proyectos como [MONAI](https://github.com/Project-MONAI/MONAI) que lo usa ampliamente.

A continuaci√≥n se muestra la arquitectura de Densenet121:
<p align="center">
  <img align="center" src="misc/densenet.jpeg" alt="Arquitectura Densenet121" width="60%">
</p>

### Filtrado M√©dico üö´

El filtrado m√©dico es la tarea de etiquetar im√°genes en dos conjuntos, m√©dicos y no m√©dicos, ya que queremos filtrar todos los no m√©dicos para que no se incluyan en los otros modelos de aprendizaje autom√°tico.

Nuestro modelo utiliza la misma arquitectura Densenet121 del m√≥dulo [torchvision](https://pytorch.org/docs/stable/torchvision/models.html).

## Datasets ‚úîÔ∏è

Los conjuntos de datos utilizados en este proyecto son una versi√≥n aumentada de:
* [VQA-RAD](https://www.nature.com/articles/sdata2018251#data-citations)
* [Tiny ImageNet](https://tiny-imagenet.herokuapp.com/)
* [Medical Decathlon](http://medicaldecathlon.com/)
* [Mednist](https://www.dropbox.com/s/5wwskxctvcxiuea/MedNIST.tar.gz) - el conjunto de datos est√° amablemente disponible por  [Dr. Bradley J. Erickson M.D., Ph.D.](https://www.mayo.edu/research/labs/radiology-informatics/overview) (Departamento de Radiolog√≠a, Cl√≠nica Mayo) bajo licencia Creative Commons [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/).
